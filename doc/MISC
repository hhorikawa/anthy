This document is written in Japanese.

Anthy

** 本ドキュメントの構成 **
Anthyのドキュメントはパッケージの doc/ 以下に置かれ、
ドキュメントの目次は doc/00INDEX にあります。


** 本ソフトウェアの構成 **
Anthyは変換エンジンの機能などを持つライブラリと、それを利用
するためのいくつかのコマンドなどから構成されています。


** 設定 **
anthy-confに変数名、内容を記述する
anthy-confは典型的には /usr/local/etc/ にインストールされる
変数名には ANTHYDIR, DIC_FILE, INDEPWORD, DEPWORD, (ZIPDICT_EUC)


** 変換エンジンの構成 **
Anthyはおおまかにわけて次の4つのモジュールで構成されています。
 *辞書モジュール src-diclib/
 *文節境界のモジュール src-splitter/
 *候補の順序の決定のモジュール src-ordering/
 *その他(候補の割当て、変換コンテキストの管理等々) src-main/

日本語にはある読みに対して複数の意味や品詞がある同音異義語が
多数存在します。これを扱うために、変換元の文字列(つまり同音の
文字列)を代表するためのデータ型を用意しています。
このデータ型を用いたプログラムの典型的な流れを次に示します。

int n, i;
 /* ある読みに対応するハンドルを取得する */
seq_ent_t se = get_seq_ent_from_xstr(yomi_string);
 /* いくつの同音異義語が存在するかを取得 */
n = get_nr_dic_ents(se, NULL);
 /* 各異義語に対して */
for (i = 0; i < n; i++) {
   /* 単語を取得する */
  get_nth_dic_ent_str(se, NULL, i, &str);
   /* 品詞を取得する */
  get_nth_dic_ent_wtype(se, NULL, i, &wt);
 ..
}

同音異義語の中には同じ品詞のものや異なる品詞をもつものもあり、
活用する単語の各種の活用形が含まれます。上記のプログラムでは
いくつの同音異義語があるかを取得したあとに、インデックスを用い
て各種の情報を取得しています。

かな漢字変換の各モジュールはこの辞書ライブラリからデータを
取得することによって動作します。次に各モジュールのおおまかな
構成を説明します。

Anthyライブラリの動作はユーザが変換前の(主に平仮名で構成される)
文字列を渡すことによって開始されます。変換の動作は文節の境界の
検出と候補の生成の2つにわかれます。
文節の境界はユーザが変更することがあるので、境界検出モジュールは
ユーザが設定した境界を制約条件として動作します。最初に文字列を
設定した際にはこの制約条件は無い状態です。
最初に文字列を設定した際に、文字列中の全ての部分文字列に対し
それが自立語かどうかをチェックして自立語ならば接頭辞や接尾辞
付属語を追加したパターンが文中にあるかどうかをチェックして
struct word_listというデータ構造を構成します。
どのような付属語が接続するかは自立語の品詞によって異なり、
そのデータは定義ファイルに記述されています。

候補の生成は上の分割の際に与えられた品詞をもとにして
行われます。

** コーディング規則 **
あんまり規則は作っても(私が)守らんので作らないが、以下のことぐらいを
守りたい。
1, ifやforのあとに文を一つしか書かなくても中括弧を使うこと(Perlと同じように)
2, NULLと書かずに0と書いても良い。(書けという根拠は無いです)
3, 関数のプロトタイプ宣言時に引数の変数名を書く
4, 基本的に成功時の返り値は0、失敗時は-1とする
5, 思い出したら、コード中にはきちんとスペースを入れるようにする。
6, ディレクトリ構成は浅くする


** 変換モジュールの構成 **

文節境界の設定
(文節中の単語境界の検索)
候補の列挙
候補のソート


*変換パイプライン ( コアは src-main/ 以下にある )
 context 変換コンテキスト
  wordsplit 文節境界の検出 ( src-splitter/ 以下にある )
   depgraph
   metaword
   evalborder
   hmm
  compose 文節構造の解析
  candsort 候補の並びかえ
  commit 
  candswap
 personality パーソナリティの管理
*辞書ライブラリ
 conf グローバルな設定
 file_dic 辞書ファイル
 mem_dic メモリ上の辞書
 record ユーザごとの設定
 alloc メモリアロケータ
 logger
 dic_session 辞書エントリのbook keeping
 xchar 文字型
 xstr 文字列
 wtype 品詞
 ext_ent 候補の生成
 ruleparser 設定ファイルの解析
 dic_util 辞書管理プログラムのための関数群


** seq_entについて **
anthyではひらがな列(読み)に対応するデータ構造として、seq_entという
構造体を用います。この構造体はdiclibの中で管理されており、
フロントエンド側からは適切なAPIでアクセスすることができます。
seq_entは同音異義語の配列を持っています。
 seq_entの主要メンバ
  * str: 読み
  * id: 辞書中のインデックス
  * seq_type: ST_ のフラグ
  * flags: F_ のフラグ
  * dic_ents: 単語の配列
  * compound_ents: 複合語の配列


** 辞書ファイルの形式 **
辞書ファイルはエンディアンやバイナリフォーマットの異なるマシン間で
共有できるような形式とする。
辞書のセクションは次のようなものがある
*ヘッダ
*辞書のエントリ
*辞書のエントリへのインデックス
*辞書のインデックス
*辞書のインデックスへのインデックス


** TODO **
*変換効率の向上
 付属語グラフの整理
 接頭辞接尾辞のパターン
  たかだか千数百個なので、専用のパターン定義言語をもつ？
*その他
 補完
 逆変換
 ユニコード化
 YAML導入
*設定能力
*API
  逆説的ですが、普及するまで互換性の無い変更はできません。


** デバッグの方法 **
anthy-agentは標準入力でユーザとやりとりすることを
利用して次のようなスクリプトを書きます．
--
#! /bin/sh

anthy-agent <<EOF
aho
(space)
 PRINT_CONTEXT
EOF
--
このスクリプトを実行すると手元のanthyでは次のような出力が得られます．
--
(2 ((UL) "あほ" -1 -1) cursor)
(3 ((UL RV) "アホ" 0 3))
|あほ
あほ(アホ:(1N,256)22,485 ,阿呆:(1,256)22,357 ,あほ:(1N,256)22,229 ,):
--
これはemacsでahoと入力して変換動作をする(スペースを押す)操作をしたあとで
コンテキストを表示するコマンドを入れています．

出力の1行目と2行目はプリエディットの変化を表しています．
3行目と4行目はPRINT_CONTEXTの結果です．

 文全体   -> |あほ
 一文節目 -> あほ(アホ:(1N,256)22,485 ,阿呆:(1,256)22,357 ,あほ:(1N,256)22,229 ,):

各候補は   変換結果:(候補のタイプ,文節構造のスコア)文節のスコア
という形式をしています．

anthyのソース中にはanthy_print_*() 系の関数がコメントアウトされている部分が
たくさんあるので，それを有効にして同じことをやると変換パイプラインでどんな
データが流れるかがわかると思います．

ちなみに，デバッグプリントなどを有効にしてmake installすると
anthy.elとのプロトコルに反するので注意しませう．


** Probation Queue **
開発中に不完全なコードが多く生じるのは一般に良いことですが，
残念ながら，そのまま放置されてしまう場合も多々あります．
そのようなコードは，新たにそのソフトウェアの開発に
取り掛かろうとする人に対する障壁となるので，除去
されないといけません．

以下の項目は2年以上更新がない場合には除去することにします

2002 4/30 yusuke
*agent.cのマルチコネクション機能
*anthy/Makefile.am のtestsuiteやDEJAGNUの関係

2002 5/1 yusuke
*src-ordering/relation.c

ほかにもあるハズですが，とりあえずこんだけ

2003 7/13 yusuke
long long型が存在することを想定
long longが64bit以上であることを想定
まだstdint.hは普及していないことを想定
